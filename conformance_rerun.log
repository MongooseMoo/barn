..............................s......................................... [  4%]
........................................................................ [  9%]
........................................................................ [ 14%]
............................................F........................... [ 19%]
.....................................................................sss [ 24%]
ssssssssssssssssssssssssssssssssssssssssssFFFFFFFFFFFFFFFFFFFFFFFFFFFF.. [ 29%]
........................................................................ [ 34%]
...........................................F.........FFF................ [ 38%]
....................................................F..F.s.FF....F..s... [ 43%]
.ss.....ss............................ssss....FFF..F....FF...FF......... [ 48%]
........................F.......................F.....s................. [ 53%]
.FFFF...FFFF.Fs..F...F..s..FF.F.sFF.F.FFs...F..s..sF..ss..Fs............ [ 58%]
FF......ssss..sssssssss...........................................sssFF. [ 63%]
.......................F..FFFFFF.FFFFFFsFFFFF.F.s...F.s..FF.s...F.s..sF. [ 68%]
ss..F.s...F.s...Fs..s.....s.......................................F..... [ 73%]
.sss..........................................................F......... [ 77%]
...........F............................................................ [ 82%]
................ssss...........................................sssssssss [ 87%]
sss............ss.s...............................FF...ssssss.........FF [ 92%]
.FFFFssss.F.F.F.F.......F...FF..s.....F.........................F....... [ 97%]
..............FFFFFFF.................                                   [100%]
================================== FAILURES ===================================
__ TestConformance.test_yaml_case[caller_perms::caller_perms_top_level_eval] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E466E90>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='caller_perms', description='Tests for caller_perms() builtin', version='1.0', skip=False, requires...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = '#3', actual = '#-1', test_name = 'caller_perms_top_level_eval'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'caller_perms_top_level_eval' expected value '#3', but got '#-1'

tests\conformance\runner.py:285: AssertionError
______ TestConformance.test_yaml_case[gc::run_gc_requires_wizard_perms] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491B30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'run_gc_requires_wizard_perms'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'run_gc_requires_wizard_perms' expected error E_PERM, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
__________ TestConformance.test_yaml_case[gc::run_gc_allows_wizard] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491B80>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='run_gc_allows_wizard', description='', skip=False, skip_if=None, permission='wizard', setup=None, te...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'run_gc_allows_wizard' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_____ TestConformance.test_yaml_case[gc::gc_stats_requires_wizard_perms] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491BD0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'gc_stats_requires_wizard_perms'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_requires_wizard_perms' expected error E_PERM, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_________ TestConformance.test_yaml_case[gc::gc_stats_allows_wizard] __________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491C20>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_allows_wizard', description='', skip=False, skip_if=None, permission='wizard', setup=None, ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_allows_wizard' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
__________ TestConformance.test_yaml_case[gc::gc_stats_returns_map] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491C70>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_returns_map', description='', skip=False, skip_if=None, permission='wizard', setup=None, te...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_returns_map' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_________ TestConformance.test_yaml_case[gc::gc_stats_has_purple_key] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491CC0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_has_purple_key', description='', skip=False, skip_if=None, permission='wizard', setup=None,...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_has_purple_key' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_________ TestConformance.test_yaml_case[gc::gc_stats_has_black_key] __________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491D10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_has_black_key', description='', skip=False, skip_if=None, permission='wizard', setup=None, ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_has_black_key' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_________ TestConformance.test_yaml_case[gc::gc_stats_purple_is_int] __________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491D60>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_purple_is_int', description='', skip=False, skip_if=None, permission='wizard', setup=None, ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_purple_is_int' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
__________ TestConformance.test_yaml_case[gc::gc_stats_black_is_int] __________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491DB0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='gc_stats_black_is_int', description='', skip=False, skip_if=None, permission='wizard', setup=None, t...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'gc_stats_black_is_int' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
______ TestConformance.test_yaml_case[gc::nested_list_no_possible_root] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E491E00>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='nested_list_no_possible_root', description='', skip=False, skip_if=None, permission='wizard', setup=...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'nested_list_no_possible_root' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_______ TestConformance.test_yaml_case[gc::nested_map_no_possible_root] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4923F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='nested_map_no_possible_root', description='', skip=False, skip_if=None, permission='wizard', setup=N...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'nested_map_no_possible_root' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[gc::run_gc_doesnt_crash_with_anonymous] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AEE90>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='run_gc_doesnt_crash_with_anonymous', description='', skip=False, skip_if=None, permission='wizard', ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'run_gc_doesnt_crash_with_anonymous' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[gc::single_cyclic_self_reference_basic] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AEEE0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='single_cyclic_self_reference_basic', description='', skip=False, skip_if=None, permission='wizard', ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'single_cyclic_self_reference_basic' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[gc::single_cyclic_self_reference_with_recycle] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AEF30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='single_cyclic_self_reference_with_recycle', description='', skip=False, skip_if=None, permission='wi...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'single_cyclic_self_reference_with_recycle' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_______ TestConformance.test_yaml_case[gc::dual_cyclic_self_references] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AEF80>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='dual_cyclic_self_references', description='', skip=False, skip_if=None, permission='wizard', setup=N...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'dual_cyclic_self_references' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[gc::dual_cyclic_self_references_with_recycle] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AEFD0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='dual_cyclic_self_references_with_recycle', description='', skip=False, skip_if=None, permission='wiz...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'dual_cyclic_self_references_with_recycle' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_____ TestConformance.test_yaml_case[gc::cyclic_references_through_list] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF020>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='cyclic_references_through_list', description='', skip=False, skip_if=None, permission='wizard', setu...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'cyclic_references_through_list' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[gc::cyclic_references_through_list_with_recycle] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF070>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='cyclic_references_through_list_with_recycle', description='', skip=False, skip_if=None, permission='...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'cyclic_references_through_list_with_recycle' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
______ TestConformance.test_yaml_case[gc::cyclic_references_through_map] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF0C0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='cyclic_references_through_map', description='', skip=False, skip_if=None, permission='wizard', setup...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'cyclic_references_through_map' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[gc::cyclic_references_through_map_with_recycle] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF110>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='cyclic_references_through_map_with_recycle', description='', skip=False, skip_if=None, permission='w...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'cyclic_references_through_map_with_recycle' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
___________ TestConformance.test_yaml_case[gc::empty_list_is_green] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF1B0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='empty_list_is_green', description='', skip=False, skip_if=None, permission='wizard', setup=None, tea...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'empty_list_is_green' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
___________ TestConformance.test_yaml_case[gc::empty_map_is_green] ____________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF250>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='gc', description='Tests for garbage collection builtins (run_gc, gc_stats)', version='1.0', skip=F...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='empty_map_is_green', description='', skip=False, skip_if=None, permission='wizard', setup=None, tear...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'empty_map_is_green' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[http::non_wizard_cannot_call_no_arg_version] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF2A0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'non_wizard_cannot_call_no_arg_version'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'non_wizard_cannot_call_no_arg_version' expected error E_PERM, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
________ TestConformance.test_yaml_case[http::read_http_no_args_fails] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF2F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_ARGS'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'read_http_no_args_fails'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_http_no_args_fails' expected error E_ARGS, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_____ TestConformance.test_yaml_case[http::read_http_invalid_type_foobar] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF340>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'read_http_invalid_type_foobar'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_http_invalid_type_foobar' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
__ TestConformance.test_yaml_case[http::read_http_invalid_type_empty_string] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF390>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'read_http_invalid_type_empty_string'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_http_invalid_type_empty_string' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_____ TestConformance.test_yaml_case[http::read_http_type_arg_not_string] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF3E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'read_http_type_arg_not_string'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_http_type_arg_not_string' expected error E_TYPE, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
___ TestConformance.test_yaml_case[http::read_http_connection_arg_not_obj] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E4AF430>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='http', description='HTTP request/response parsing builtin (read_http)', version='1.0', skip=False,...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'read_http_connection_arg_not_obj'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_http_connection_arg_not_obj' expected error E_TYPE, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_______ TestConformance.test_yaml_case[json::generate_json_escape_tab] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E605B30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='json', description='JSON generation and parsing builtins (generate_json, parse_json)', version='1....e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = '{"foo":"bar\\u0009baz"}', actual = '{"foo":"bar\\tbaz"}'
test_name = 'generate_json_escape_tab'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'generate_json_escape_tab' expected value '{"foo":"bar\\u0009baz"}', but got '{"foo":"bar\\tbaz"}'

tests\conformance\runner.py:285: AssertionError
________ TestConformance.test_yaml_case[json::generate_json_anon_obj] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E605E50>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='json', description='JSON generation and parsing builtins (generate_json, parse_json)', version='1....', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value='"#597"', error=None, error_message=None, notifications=[], logs=[])
test_name = 'generate_json_anon_obj'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'generate_json_anon_obj' expected error E_INVARG, but got success with value: '"#597"'

tests\conformance\runner.py:263: AssertionError
_____ TestConformance.test_yaml_case[json::generate_json_anon_obj_common] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E605EA0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='json', description='JSON generation and parsing builtins (generate_json, parse_json)', version='1....', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value='"#599"', error=None, error_message=None, notifications=[], logs=[])
test_name = 'generate_json_anon_obj_common'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'generate_json_anon_obj_common' expected error E_INVARG, but got success with value: '"#599"'

tests\conformance\runner.py:263: AssertionError
____ TestConformance.test_yaml_case[json::generate_json_anon_obj_embedded] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E605EF0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='json', description='JSON generation and parsing builtins (generate_json, parse_json)', version='1....', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value='"#601|obj"', error=None, error_message=None, notifications=[], logs=[])
test_name = 'generate_json_anon_obj_embedded'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'generate_json_anon_obj_embedded' expected error E_INVARG, but got success with value: '"#601|obj"'

tests\conformance\runner.py:263: AssertionError
________ TestConformance.test_yaml_case[map::mapdelete_empty_list_key] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E607B10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='map', description='Map literal syntax, operations, and builtin functions', version='1.0', skip=Fal...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='mapdelete_empty_list_key', description='Empty list is a valid map key - returns map unchanged since ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_TYPE: 'E_TYPE'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'mapdelete_empty_list_key' expected success but got error: MooError.E_TYPE

tests\conformance\runner.py:231: AssertionError
_______ TestConformance.test_yaml_case[map::mapdelete_list_values_key] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E607CA0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='map', description='Map literal syntax, operations, and builtin functions', version='1.0', skip=Fal...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_RANGE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_TYPE: 'E_TYPE'>, error_message=None, notifications=[], logs=[])
test_name = 'mapdelete_list_values_key'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'mapdelete_list_values_key' expected error E_RANGE, but got E_TYPE

tests\conformance\runner.py:277: AssertionError
_______ TestConformance.test_yaml_case[map::ranged_set_invalid_range_2] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E607E30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='map', description='Map literal syntax, operations, and builtin functions', version='1.0', skip=Fal...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_RANGE'
result = ExecutionResult(success=True, value={1: 1, 'a': 'a', 'b': 'b'}, error=None, error_message=None, notifications=[], logs=[])
test_name = 'ranged_set_invalid_range_2'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'ranged_set_invalid_range_2' expected error E_RANGE, but got success with value: {1: 1, 'a': 'a', 'b': 'b'}

tests\conformance\runner.py:263: AssertionError
_______ TestConformance.test_yaml_case[map::ranged_set_invalid_range_3] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E607E80>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='map', description='Map literal syntax, operations, and builtin functions', version='1.0', skip=Fal...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_RANGE'
result = ExecutionResult(success=True, value={1: 1, 'a': 'a', 'b': 'b'}, error=None, error_message=None, notifications=[], logs=[])
test_name = 'ranged_set_invalid_range_3'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'ranged_set_invalid_range_3' expected error E_RANGE, but got success with value: {1: 1, 'a': 'a', 'b': 'b'}

tests\conformance\runner.py:263: AssertionError
______ TestConformance.test_yaml_case[map::inverted_ranged_set_in_loop] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E664050>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='map', description='Map literal syntax, operations, and builtin functions', version='1.0', skip=Fal...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_RANGE'
result = ExecutionResult(success=True, value=10, error=None, error_message=None, notifications=[], logs=[])
test_name = 'inverted_ranged_set_in_loop'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'inverted_ranged_set_in_loop' expected error E_RANGE, but got success with value: 10

tests\conformance\runner.py:263: AssertionError
____ TestConformance.test_yaml_case[objects::create_invalid_owner_invarg] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6650E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='create_invalid_owner_invarg', description='', skip=False, skip_if=None, permission='wizard', setup=N...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_owner_invarg' expected success but got error: MooError.E_INVARG

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[objects::create_invalid_owner_ambiguous] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E665130>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='create_invalid_owner_ambiguous', description='', skip=False, skip_if=None, permission='wizard', setu...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_owner_ambiguous' expected success but got error: MooError.E_INVARG

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[objects::create_invalid_owner_failed_match] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E665180>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='create_invalid_owner_failed_match', description='', skip=False, skip_if=None, permission='wizard', s...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_owner_failed_match' expected success but got error: MooError.E_INVARG

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[objects::create_invalid_owner_invarg_as_programmer] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E665270>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'create_invalid_owner_invarg_as_programmer'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_owner_invarg_as_programmer' expected error E_PERM, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
__ TestConformance.test_yaml_case[objects::create_invalid_parent_ambiguous] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E665450>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'create_invalid_parent_ambiguous'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_parent_ambiguous' expected error E_TYPE, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[objects::create_invalid_parent_failed_match] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6654A0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'create_invalid_parent_failed_match'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_invalid_parent_failed_match' expected error E_TYPE, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
___ TestConformance.test_yaml_case[objects::create_list_invalid_ambiguous] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6655E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'create_list_invalid_ambiguous'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_list_invalid_ambiguous' expected error E_TYPE, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
__ TestConformance.test_yaml_case[objects::create_list_invalid_failed_match] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E665630>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'create_list_invalid_failed_match'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'create_list_invalid_failed_match' expected error E_TYPE, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
___________ TestConformance.test_yaml_case[objects::renumber_basic] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E666120>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='renumber_basic', description='', skip=False, skip_if=None, permission='wizard', setup=None, teardown...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_TYPE: 'E_TYPE'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'renumber_basic' expected success but got error: MooError.E_TYPE

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[objects::chparent_property_conflict_variations] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6668F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='objects', description='Tests for MOO object system - create, recycle, parent/child relationships, ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=[1, 1, 1], error=None, error_message=None, notifications=[], logs=[])
test_name = 'chparent_property_conflict_variations'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'chparent_property_conflict_variations' expected error E_INVARG, but got success with value: [1, 1, 1]

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[primitives::queued_tasks_includes_this_map] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6670C0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='primitives', description='Primitive value prototypes and method dispatch', version='1.0', skip=Fal...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='queued_tasks_includes_this_map', description='', skip=False, skip_if=None, permission='wizard', setu...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'queued_tasks_includes_this_map' expected success but got error: MooError.E_VARNF

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[primitives::callers_includes_this_list] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667110>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='primitives', description='Primitive value prototypes and method dispatch', version='1.0', skip=Fal...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = [1, 2, 'three', 4.0], actual = '#1058'
test_name = 'callers_includes_this_list'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'callers_includes_this_list' expected value [1, 2, 'three', 4.0], but got '#1058'

tests\conformance\runner.py:285: AssertionError
___ TestConformance.test_yaml_case[primitives::inheritance_with_prototypes] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667160>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='primitives', description='Primitive value prototypes and method dispatch', version='1.0', skip=Fal...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='inheritance_with_prototypes', description='', skip=False, skip_if=None, permission='wizard', setup=S...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_TYPE: 'E_TYPE'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'inheritance_with_prototypes' expected success but got error: MooError.E_TYPE

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[primitives::pass_works_with_prototypes] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6671B0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='primitives', description='Primitive value prototypes and method dispatch', version='1.0', skip=Fal...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='pass_works_with_prototypes', description='', skip=False, skip_if=None, permission='wizard', setup=Se...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_TYPE: 'E_TYPE'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'pass_works_with_prototypes' expected success but got error: MooError.E_TYPE

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[properties::add_property_invalid_owner] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6672F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({obj}, "foobar", ...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({obj}, "foobar", ...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
___ TestConformance.test_yaml_case[properties::add_property_invalid_perms] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667340>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({obj}, "foobar", ...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({obj}, "foobar", ...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
__ TestConformance.test_yaml_case[properties::add_property_recycled_object] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667390>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({obj}, "foobar", ...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({obj}, "foobar", ...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[properties::add_property_builtin_name] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6673E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({obj}, "name", 0,...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({obj}, "name", 0,...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[properties::add_property_defined_on_descendant] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667480>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...one, as_=None, expect=None), TestStep(run='recycle({parent})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({parent}, "foobar...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({parent}, "foobar...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_____ TestConformance.test_yaml_case[properties::add_property_not_owner] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6675C0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_property({obj}, "foobar", ...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_property({obj}, "foobar", ...'' expected error E_PERM, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[properties::delete_property_recycled_object] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667700>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'delete_property({obj}, "foobar...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'delete_property({obj}, "foobar...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
_____ TestConformance.test_yaml_case[properties::is_clear_property_works] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6678E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...one, as_=None, expect=None), TestStep(run='recycle({parent})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'is_clear_property({child}, "fo...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'is_clear_property({child}, "fo...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[properties::is_clear_property_recycled_object] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667930>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'is_clear_property({obj}, "foob...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'is_clear_property({obj}, "foob...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[properties::is_clear_property_builtin] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6679D0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expect = Expectation(value=0, error=None, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None)
result = ExecutionResult(success=False, value=None, error=<MooError.E_PROPNF: 'E_PROPNF'>, error_message=None, notifications=[], logs=[])
context = 'step \'is_clear_property({obj}, "name...\''

    def _verify_expectation(self, expect: Expectation, result: ExecutionResult, context: str) -> None:
        """Verify a single expectation against a result.
    
        Args:
            expect: The expectation to verify
            result: The execution result
            context: Context string for error messages (e.g., test name or step description)
        """
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, context)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"{context} expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: step 'is_clear_property({obj}, "name...' expected success but got error: MooError.E_PROPNF

tests\conformance\runner.py:195: AssertionError
_ TestConformance.test_yaml_case[properties::is_clear_property_with_read_permission] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667AC0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...one, as_=None, expect=None), TestStep(run='recycle({parent})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'is_clear_property({child}, "fo...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'is_clear_property({child}, "fo...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[properties::is_clear_property_wizard_bypasses_read] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667B10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...one, as_=None, expect=None), TestStep(run='recycle({parent})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'is_clear_property({child}, "fo...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'is_clear_property({child}, "fo...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[properties::clear_property_recycled_object] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667BB0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'clear_property({obj}, "foobar"...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'clear_property({obj}, "foobar"...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
_____ TestConformance.test_yaml_case[properties::clear_property_builtin] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667C50>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_PROPNF: 'E_PROPNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'clear_property({obj}, "name")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'clear_property({obj}, "name")...'' expected error E_PERM, but got E_PROPNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[properties::clear_property_on_definer] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667CA0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'clear_property({obj}, "foobar"...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'clear_property({obj}, "foobar"...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
__ TestConformance.test_yaml_case[properties::property_info_recycled_object] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E667E30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_PROPNF: 'E_PROPNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'property_info({obj}, "foobar")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'property_info({obj}, "foobar")...'' expected error E_INVARG, but got E_PROPNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[properties::set_property_info_recycled_object] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C40A0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'set_property_info({obj}, "foob...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'set_property_info({obj}, "foob...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
___ TestConformance.test_yaml_case[properties::properties_recycled_object] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C42D0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='property_builtins', description='Tests for property manipulation builtins - property_info, add_pro...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = "step 'properties({obj})...'"

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'properties({obj})...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[recycle::recycle_invalid_already_recycled_object] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C4730>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='recycle_builtins', description='Tests for recycle() builtin - destroying objects and anonymous obj...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'recycle_invalid_already_recycled_object'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'recycle_invalid_already_recycled_object' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[recycle::recycle_invalid_already_recycled_anonymous] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C4780>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='recycle_builtins', description='Tests for recycle() builtin - destroying objects and anonymous obj...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'recycle_invalid_already_recycled_anonymous'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'recycle_invalid_already_recycled_anonymous' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
____ TestConformance.test_yaml_case[switch_player::non_wizard_gets_E_PERM] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C5CC0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='switch_player', description='Tests for switch_player() builtin function', version='1.0', skip=Fals...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'non_wizard_gets_E_PERM'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'non_wizard_gets_E_PERM' expected error E_PERM, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[switch_player::programmer_cannot_switch_player] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C5D10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='switch_player', description='Tests for switch_player() builtin function', version='1.0', skip=Fals...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'programmer_cannot_switch_player'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'programmer_cannot_switch_player' expected error E_PERM, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
______ TestConformance.test_yaml_case[task_local::fork_and_suspend_case] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C64E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = [[1, 2], ['foo', 'bar', 3, 4, 5]], actual = [[1, 2], 0]
test_name = 'fork_and_suspend_case'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'fork_and_suspend_case' expected value [[1, 2], ['foo', 'bar', 3, 4, 5]], but got [[1, 2], 0]

tests\conformance\runner.py:285: AssertionError
__________ TestConformance.test_yaml_case[task_local::command_verb] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C65D0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...None, contains=None, range=None, satisfies=None, notifications=['> {"#-1 to the #-1"}']), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='command_verb', description='', skip=False, skip_if=None, permission='wizard', setup=None, teardown=N...=None, contains=None, range=None, satisfies=None, notifications=['> {"#-1 to the #-1"}']), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'command_verb' expected success but got error: MooError.E_INVIND

tests\conformance\runner.py:231: AssertionError
___________ TestConformance.test_yaml_case[task_local::server_verb] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6670>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...atch=None, contains=None, range=None, satisfies=None, notifications=['> {1, {2, {3}}}']), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='server_verb', description='', skip=False, skip_if=None, permission='wizard', setup=None, teardown=No...match=None, contains=None, range=None, satisfies=None, notifications=['> {1, {2, {3}}}']), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'server_verb' expected success but got error: MooError.E_INVARG

tests\conformance\runner.py:231: AssertionError
________ TestConformance.test_yaml_case[task_local::across_verb_calls] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C66C0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='across_verb_calls', description='', skip=False, skip_if=None, permission='wizard', setup=None, teard...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'across_verb_calls' expected success but got error: MooError.E_INVIND

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[task_local::across_verb_calls_with_intermediate] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6710>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='across_verb_calls_with_intermediate', description='', skip=False, skip_if=None, permission='wizard',...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'across_verb_calls_with_intermediate' expected success but got error: MooError.E_INVIND

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[task_local::suspend_between_verb_calls] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6760>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='suspend_between_verb_calls', description='', skip=False, skip_if=None, permission='wizard', setup=No...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'suspend_between_verb_calls' expected success but got error: MooError.E_INVIND

tests\conformance\runner.py:231: AssertionError
_____ TestConformance.test_yaml_case[task_local::read_between_verb_calls] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C67B0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='read_between_verb_calls', description='', skip=False, skip_if=None, permission='wizard', setup=None,...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'read_between_verb_calls' expected success but got error: MooError.E_INVIND

tests\conformance\runner.py:231: AssertionError
_____ TestConformance.test_yaml_case[task_local::nonfunctional_kill_task] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6850>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='task_local', description='Tests for task_local() and set_task_local() builtins', version='1.0', sk...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INTRPT'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVARG: 'E_INVARG'>, error_message=None, notifications=[], logs=[])
test_name = 'nonfunctional_kill_task'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'nonfunctional_kill_task' expected error E_INTRPT, but got E_INVARG

tests\conformance\runner.py:277: AssertionError
____________ TestConformance.test_yaml_case[verbs::add_verb_basic] ____________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C68F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'add_verb({obj}, {player, "x", ...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "x", ...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
________ TestConformance.test_yaml_case[verbs::add_verb_invalid_owner] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6940>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_verb({obj}, {$nothing, "",...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {$nothing, "",...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
________ TestConformance.test_yaml_case[verbs::add_verb_invalid_perms] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6990>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_verb({obj}, {player, "abc"...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "abc"...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
________ TestConformance.test_yaml_case[verbs::add_verb_invalid_args] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C69E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_verb({obj}, {player, "", "...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "", "...'' expected error E_INVARG, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
_______ TestConformance.test_yaml_case[verbs::add_verb_recycled_object] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6A30>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_verb({obj}, {player, "", "...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "", "...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[verbs::add_verb_with_write_permission] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6AD0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'add_verb({obj}, {player, "", "...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "", "...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
____ TestConformance.test_yaml_case[verbs::add_verb_wizard_bypasses_write] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6B20>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'add_verb({obj}, {player, "", "...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "", "...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
__________ TestConformance.test_yaml_case[verbs::add_verb_not_owner] __________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6B70>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=True, value=0, error=None, error_message=None, notifications=[], logs=[])
test_name = 'step \'add_verb({obj}, {$system, "", ...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {$system, "", ...'' expected error E_PERM, but got success with value: 0

tests\conformance\runner.py:263: AssertionError
__________ TestConformance.test_yaml_case[verbs::add_verb_is_owner] ___________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6BC0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'add_verb({obj}, {player, "", "...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {player, "", "...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
______ TestConformance.test_yaml_case[verbs::add_verb_wizard_sets_owner] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6C10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 1, actual = 0
test_name = 'step \'add_verb({obj}, {$system, "", ...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'add_verb({obj}, {$system, "", ...'' expected value 1, but got 0

tests\conformance\runner.py:285: AssertionError
_____ TestConformance.test_yaml_case[verbs::delete_verb_recycled_object] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6CB0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'delete_verb({obj}, "foobar")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'delete_verb({obj}, "foobar")...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
______ TestConformance.test_yaml_case[verbs::verb_info_recycled_object] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C6EE0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'verb_info({obj}, "foobar")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'verb_info({obj}, "foobar")...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
___________ TestConformance.test_yaml_case[verbs::verb_args_basic] ____________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C70C0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in..., notifications=None), cleanup=[TestStep(run='recycle({obj})', capture=None, as_=None, expect=None)], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:202: in _verify_expectation
    self._verify_value(expect.value, result.value, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = ['any', 'on top of/on/onto/upon', 'this']
actual = ['any', 'on', 'this']
test_name = 'step \'verb_args({obj}, "foobar")...\''

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'verb_args({obj}, "foobar")...'' expected value ['any', 'on top of/on/onto/upon', 'this'], but got ['any', 'on', 'this']

tests\conformance\runner.py:285: AssertionError
______ TestConformance.test_yaml_case[verbs::verb_args_recycled_object] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C7110>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'verb_args({obj}, "foobar")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'verb_args({obj}, "foobar")...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
______ TestConformance.test_yaml_case[verbs::verb_code_recycled_object] _______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C7340>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'verb_code({obj}, "foobar")...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'verb_code({obj}, "foobar")...'' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[verbs::set_verb_info_recycled_object] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C7520>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'set_verb_info({obj}, "foobar",...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'set_verb_info({obj}, "foobar",...'' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[verbs::set_verb_args_recycled_object] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C7700>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'set_verb_args({obj}, "foobar",...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'set_verb_args({obj}, "foobar",...'' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[verbs::set_verb_code_recycled_object] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C78E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'step \'set_verb_code({obj}, "foobar",...\''

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'set_verb_code({obj}, "foobar",...'' expected error E_INVARG, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
________ TestConformance.test_yaml_case[verbs::verbs_recycled_object] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E6C7B10>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='verb_builtins', description='Tests for verb manipulation builtins - add_verb, delete_verb, verb_in...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:75: in run_test
    self._execute_steps(test)
tests\conformance\runner.py:148: in _execute_steps
    self._verify_expectation(step.expect, result, f"step '{step.run[:30]}...'")
tests\conformance\runner.py:190: in _verify_expectation
    self._verify_error(expect.error, result, context)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_INVIND: 'E_INVIND'>, error_message=None, notifications=[], logs=[])
test_name = "step 'verbs({obj})...'"

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'step 'verbs({obj})...'' expected error E_INVARG, but got E_INVIND

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[anonymous::recycle_invalid_anonymous_no_crash] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E730AF0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='anonymous', description='Anonymous object creation, lifecycle, and invalidation', version='1.0', s...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=1, error=None, error_message=None, notifications=[], logs=[])
test_name = 'recycle_invalid_anonymous_no_crash'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'recycle_invalid_anonymous_no_crash' expected error E_INVARG, but got success with value: 1

tests\conformance\runner.py:263: AssertionError
_____ TestConformance.test_yaml_case[index_and_range::range_list_single] ______

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7320D0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='index_and_range', description='Index and range extensions with ^ (first) and $ (last) operators', ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = ['three'], actual = 'three', test_name = 'range_list_single'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'range_list_single' expected value ['three'], but got 'three'

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[index_and_range::decompile_with_index_operators] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E732760>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='index_and_range', description='Index and range extensions with ^ (first) and $ (last) operators', ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = ['return "foobar"[^ + 2 ^ 2..$ - $off];']
actual = ['return "foobar"[^ + 2 ^ 2 .. $ - #0.off];']
test_name = 'decompile_with_index_operators'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'decompile_with_index_operators' expected value ['return "foobar"[^ + 2 ^ 2..$ - $off];'], but got ['return "foobar"[^ + 2 ^ 2 .. $ - #0.off];']

tests\conformance\runner.py:285: AssertionError
________ TestConformance.test_yaml_case[waif::nested_waif_map_indexes] ________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796120>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='waif', description='Tests for waif (lightweight objects) functionality', version='1.0', skip=False...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='nested_waif_map_indexes', description='', skip=False, skip_if=None, permission='programmer', setup=S...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_PROPNF: 'E_PROPNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'nested_waif_map_indexes' expected success but got error: MooError.E_PROPNF

tests\conformance\runner.py:231: AssertionError
____ TestConformance.test_yaml_case[waif::deeply_nested_waif_map_indexes] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796170>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='waif', description='Tests for waif (lightweight objects) functionality', version='1.0', skip=False...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='deeply_nested_waif_map_indexes', description='', skip=False, skip_if=None, permission='programmer', ...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_PROPNF: 'E_PROPNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'deeply_nested_waif_map_indexes' expected success but got error: MooError.E_PROPNF

tests\conformance\runner.py:231: AssertionError
_________ TestConformance.test_yaml_case[exec::exec_with_sleep_works] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796800>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = [0, '1~0A2~0A3~0A', ''], actual = [0, '1\r\n2\r\n3\r\n', '']
test_name = 'exec_with_sleep_works'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'exec_with_sleep_works' expected value [0, '1~0A2~0A3~0A', ''], but got [0, '1\r\n2\r\n3\r\n', '']

tests\conformance\runner.py:285: AssertionError
__ TestConformance.test_yaml_case[exec::exec_rejects_invalid_binary_string] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796850>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=[0, '1~ZZ23~0A', '1~ZZ23~0A'], error=None, error_message=None, notifications=[], logs=[])
test_name = 'exec_rejects_invalid_binary_string'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'exec_rejects_invalid_binary_string' expected error E_INVARG, but got success with value: [0, '1~ZZ23~0A', '1~ZZ23~0A']

tests\conformance\runner.py:263: AssertionError
___ TestConformance.test_yaml_case[exec::kill_task_works_on_suspended_exec] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7968F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='kill_task_works_on_suspended_exec', description='', skip=False, skip_if=None, permission='wizard', s...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'kill_task_works_on_suspended_exec' expected success but got error: MooError.E_VARNF

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[exec::kill_task_fails_on_already_killed] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796940>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])
test_name = 'kill_task_fails_on_already_killed'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'kill_task_fails_on_already_killed' expected error E_INVARG, but got E_VARNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[exec::resume_fails_on_suspended_exec] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796990>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])
test_name = 'resume_fails_on_suspended_exec'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'resume_fails_on_suspended_exec' expected error E_INVARG, but got E_VARNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[exec::suspended_exec_task_stack_matches_suspended_task] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7969E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='suspended_exec_task_stack_matches_suspended_task', description='', skip=False, skip_if=None, permiss...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'suspended_exec_task_stack_matches_suspended_task' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
_ TestConformance.test_yaml_case[limits::setadd_checks_list_max_value_bytes_exceeds] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796BC0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=1472, error=None, error_message=None, notifications=[], logs=[])
test_name = 'setadd_checks_list_max_value_bytes_exceeds'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'setadd_checks_list_max_value_bytes_exceeds' expected error E_QUOTA, but got success with value: 1472

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[limits::listinsert_checks_list_max_value_bytes] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796C60>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=1472, error=None, error_message=None, notifications=[], logs=[])
test_name = 'listinsert_checks_list_max_value_bytes'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'listinsert_checks_list_max_value_bytes' expected error E_QUOTA, but got success with value: 1472

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[limits::listappend_checks_list_max_value_bytes] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796D00>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=1472, error=None, error_message=None, notifications=[], logs=[])
test_name = 'listappend_checks_list_max_value_bytes'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'listappend_checks_list_max_value_bytes' expected error E_QUOTA, but got success with value: 1472

tests\conformance\runner.py:263: AssertionError
__ TestConformance.test_yaml_case[limits::listset_fails_if_value_too_large] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E796DA0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=2928, error=None, error_message=None, notifications=[], logs=[])
test_name = 'listset_fails_if_value_too_large'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'listset_fails_if_value_too_large' expected error E_QUOTA, but got success with value: 2928

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[limits::decode_binary_checks_list_max_value_bytes] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E797020>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=['x', 0, 13, 10, 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx...xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'], error=None, error_message=None, notifications=[], logs=[])
test_name = 'decode_binary_checks_list_max_value_bytes'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'decode_binary_checks_list_max_value_bytes' expected error E_QUOTA, but got success with value: ['x', 0, 13, 10, 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx']

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[limits::list_literal_checks_max_value_bytes] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E797160>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=[0, 'E_QUOTA'], error=None, error_message=None, notifications=[], logs=[])
test_name = 'list_literal_checks_max_value_bytes'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'list_literal_checks_max_value_bytes' expected error E_QUOTA, but got success with value: [0, 'E_QUOTA']

tests\conformance\runner.py:263: AssertionError
_ TestConformance.test_yaml_case[limits::map_literal_checks_max_value_bytes] __

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7971B0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_QUOTA'
result = ExecutionResult(success=True, value=[1, {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 1...: 995, 996: 996, 997: 997, 998: 998, 999: 999, 1000: 1000}], error=None, error_message=None, notifications=[], logs=[])
test_name = 'map_literal_checks_max_value_bytes'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'map_literal_checks_max_value_bytes' expected error E_QUOTA, but got success with value: [1, {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99, 100: 100, 101: 101, 102: 102, 103: 103, 104: 104, 105: 105, 106: 106, 107: 107, 108: 108, 109: 109, 110: 110, 111: 111, 112: 112, 113: 113, 114: 114, 115: 115, 116: 116, 117: 117, 118: 118, 119: 119, 120: 120, 121: 121, 122: 122, 123: 123, 124: 124, 125: 125, 126: 126, 127: 127, 128: 128, 129: 129, 130: 130, 131: 131, 132: 132, 133: 133, 134: 134, 135: 135, 136: 136, 137: 137, 138: 138, 139: 139, 140: 140, 141: 141, 142: 142, 143: 143, 144: 144, 145: 145, 146: 146, 147: 147, 148: 148, 149: 149, 150: 150, 151: 151, 152: 152, 153: 153, 154: 154, 155: 155, 156: 156, 157: 157, 158: 158, 159: 159, 160: 160, 161: 161, 162: 162, 163: 163, 164: 164, 165: 165, 166: 166, 167: 167, 168: 168, 169: 169, 170: 170, 171: 171, 172: 172, 173: 173, 174: 174, 175: 175, 176: 176, 177: 177, 178: 178, 179: 179, 180: 180, 181: 181, 182: 182, 183: 183, 184: 184, 185: 185, 186: 186, 187: 187, 188: 188, 189: 189, 190: 190, 191: 191, 192: 192, 193: 193, 194: 194, 195: 195, 196: 196, 197: 197, 198: 198, 199: 199, 200: 200, 201: 201, 202: 202, 203: 203, 204: 204, 205: 205, 206: 206, 207: 207, 208: 208, 209: 209, 210: 210, 211: 211, 212: 212, 213: 213, 214: 214, 215: 215, 216: 216, 217: 217, 218: 218, 219: 219, 220: 220, 221: 221, 222: 222, 223: 223, 224: 224, 225: 225, 226: 226, 227: 227, 228: 228, 229: 229, 230: 230, 231: 231, 232: 232, 233: 233, 234: 234, 235: 235, 236: 236, 237: 237, 238: 238, 239: 239, 240: 240, 241: 241, 242: 242, 243: 243, 244: 244, 245: 245, 246: 246, 247: 247, 248: 248, 249: 249, 250: 250, 251: 251, 252: 252, 253: 253, 254: 254, 255: 255, 256: 256, 257: 257, 258: 258, 259: 259, 260: 260, 261: 261, 262: 262, 263: 263, 264: 264, 265: 265, 266: 266, 267: 267, 268: 268, 269: 269, 270: 270, 271: 271, 272: 272, 273: 273, 274: 274, 275: 275, 276: 276, 277: 277, 278: 278, 279: 279, 280: 280, 281: 281, 282: 282, 283: 283, 284: 284, 285: 285, 286: 286, 287: 287, 288: 288, 289: 289, 290: 290, 291: 291, 292: 292, 293: 293, 294: 294, 295: 295, 296: 296, 297: 297, 298: 298, 299: 299, 300: 300, 301: 301, 302: 302, 303: 303, 304: 304, 305: 305, 306: 306, 307: 307, 308: 308, 309: 309, 310: 310, 311: 311, 312: 312, 313: 313, 314: 314, 315: 315, 316: 316, 317: 317, 318: 318, 319: 319, 320: 320, 321: 321, 322: 322, 323: 323, 324: 324, 325: 325, 326: 326, 327: 327, 328: 328, 329: 329, 330: 330, 331: 331, 332: 332, 333: 333, 334: 334, 335: 335, 336: 336, 337: 337, 338: 338, 339: 339, 340: 340, 341: 341, 342: 342, 343: 343, 344: 344, 345: 345, 346: 346, 347: 347, 348: 348, 349: 349, 350: 350, 351: 351, 352: 352, 353: 353, 354: 354, 355: 355, 356: 356, 357: 357, 358: 358, 359: 359, 360: 360, 361: 361, 362: 362, 363: 363, 364: 364, 365: 365, 366: 366, 367: 367, 368: 368, 369: 369, 370: 370, 371: 371, 372: 372, 373: 373, 374: 374, 375: 375, 376: 376, 377: 377, 378: 378, 379: 379, 380: 380, 381: 381, 382: 382, 383: 383, 384: 384, 385: 385, 386: 386, 387: 387, 388: 388, 389: 389, 390: 390, 391: 391, 392: 392, 393: 393, 394: 394, 395: 395, 396: 396, 397: 397, 398: 398, 399: 399, 400: 400, 401: 401, 402: 402, 403: 403, 404: 404, 405: 405, 406: 406, 407: 407, 408: 408, 409: 409, 410: 410, 411: 411, 412: 412, 413: 413, 414: 414, 415: 415, 416: 416, 417: 417, 418: 418, 419: 419, 420: 420, 421: 421, 422: 422, 423: 423, 424: 424, 425: 425, 426: 426, 427: 427, 428: 428, 429: 429, 430: 430, 431: 431, 432: 432, 433: 433, 434: 434, 435: 435, 436: 436, 437: 437, 438: 438, 439: 439, 440: 440, 441: 441, 442: 442, 443: 443, 444: 444, 445: 445, 446: 446, 447: 447, 448: 448, 449: 449, 450: 450, 451: 451, 452: 452, 453: 453, 454: 454, 455: 455, 456: 456, 457: 457, 458: 458, 459: 459, 460: 460, 461: 461, 462: 462, 463: 463, 464: 464, 465: 465, 466: 466, 467: 467, 468: 468, 469: 469, 470: 470, 471: 471, 472: 472, 473: 473, 474: 474, 475: 475, 476: 476, 477: 477, 478: 478, 479: 479, 480: 480, 481: 481, 482: 482, 483: 483, 484: 484, 485: 485, 486: 486, 487: 487, 488: 488, 489: 489, 490: 490, 491: 491, 492: 492, 493: 493, 494: 494, 495: 495, 496: 496, 497: 497, 498: 498, 499: 499, 500: 500, 501: 501, 502: 502, 503: 503, 504: 504, 505: 505, 506: 506, 507: 507, 508: 508, 509: 509, 510: 510, 511: 511, 512: 512, 513: 513, 514: 514, 515: 515, 516: 516, 517: 517, 518: 518, 519: 519, 520: 520, 521: 521, 522: 522, 523: 523, 524: 524, 525: 525, 526: 526, 527: 527, 528: 528, 529: 529, 530: 530, 531: 531, 532: 532, 533: 533, 534: 534, 535: 535, 536: 536, 537: 537, 538: 538, 539: 539, 540: 540, 541: 541, 542: 542, 543: 543, 544: 544, 545: 545, 546: 546, 547: 547, 548: 548, 549: 549, 550: 550, 551: 551, 552: 552, 553: 553, 554: 554, 555: 555, 556: 556, 557: 557, 558: 558, 559: 559, 560: 560, 561: 561, 562: 562, 563: 563, 564: 564, 565: 565, 566: 566, 567: 567, 568: 568, 569: 569, 570: 570, 571: 571, 572: 572, 573: 573, 574: 574, 575: 575, 576: 576, 577: 577, 578: 578, 579: 579, 580: 580, 581: 581, 582: 582, 583: 583, 584: 584, 585: 585, 586: 586, 587: 587, 588: 588, 589: 589, 590: 590, 591: 591, 592: 592, 593: 593, 594: 594, 595: 595, 596: 596, 597: 597, 598: 598, 599: 599, 600: 600, 601: 601, 602: 602, 603: 603, 604: 604, 605: 605, 606: 606, 607: 607, 608: 608, 609: 609, 610: 610, 611: 611, 612: 612, 613: 613, 614: 614, 615: 615, 616: 616, 617: 617, 618: 618, 619: 619, 620: 620, 621: 621, 622: 622, 623: 623, 624: 624, 625: 625, 626: 626, 627: 627, 628: 628, 629: 629, 630: 630, 631: 631, 632: 632, 633: 633, 634: 634, 635: 635, 636: 636, 637: 637, 638: 638, 639: 639, 640: 640, 641: 641, 642: 642, 643: 643, 644: 644, 645: 645, 646: 646, 647: 647, 648: 648, 649: 649, 650: 650, 651: 651, 652: 652, 653: 653, 654: 654, 655: 655, 656: 656, 657: 657, 658: 658, 659: 659, 660: 660, 661: 661, 662: 662, 663: 663, 664: 664, 665: 665, 666: 666, 667: 667, 668: 668, 669: 669, 670: 670, 671: 671, 672: 672, 673: 673, 674: 674, 675: 675, 676: 676, 677: 677, 678: 678, 679: 679, 680: 680, 681: 681, 682: 682, 683: 683, 684: 684, 685: 685, 686: 686, 687: 687, 688: 688, 689: 689, 690: 690, 691: 691, 692: 692, 693: 693, 694: 694, 695: 695, 696: 696, 697: 697, 698: 698, 699: 699, 700: 700, 701: 701, 702: 702, 703: 703, 704: 704, 705: 705, 706: 706, 707: 707, 708: 708, 709: 709, 710: 710, 711: 711, 712: 712, 713: 713, 714: 714, 715: 715, 716: 716, 717: 717, 718: 718, 719: 719, 720: 720, 721: 721, 722: 722, 723: 723, 724: 724, 725: 725, 726: 726, 727: 727, 728: 728, 729: 729, 730: 730, 731: 731, 732: 732, 733: 733, 734: 734, 735: 735, 736: 736, 737: 737, 738: 738, 739: 739, 740: 740, 741: 741, 742: 742, 743: 743, 744: 744, 745: 745, 746: 746, 747: 747, 748: 748, 749: 749, 750: 750, 751: 751, 752: 752, 753: 753, 754: 754, 755: 755, 756: 756, 757: 757, 758: 758, 759: 759, 760: 760, 761: 761, 762: 762, 763: 763, 764: 764, 765: 765, 766: 766, 767: 767, 768: 768, 769: 769, 770: 770, 771: 771, 772: 772, 773: 773, 774: 774, 775: 775, 776: 776, 777: 777, 778: 778, 779: 779, 780: 780, 781: 781, 782: 782, 783: 783, 784: 784, 785: 785, 786: 786, 787: 787, 788: 788, 789: 789, 790: 790, 791: 791, 792: 792, 793: 793, 794: 794, 795: 795, 796: 796, 797: 797, 798: 798, 799: 799, 800: 800, 801: 801, 802: 802, 803: 803, 804: 804, 805: 805, 806: 806, 807: 807, 808: 808, 809: 809, 810: 810, 811: 811, 812: 812, 813: 813, 814: 814, 815: 815, 816: 816, 817: 817, 818: 818, 819: 819, 820: 820, 821: 821, 822: 822, 823: 823, 824: 824, 825: 825, 826: 826, 827: 827, 828: 828, 829: 829, 830: 830, 831: 831, 832: 832, 833: 833, 834: 834, 835: 835, 836: 836, 837: 837, 838: 838, 839: 839, 840: 840, 841: 841, 842: 842, 843: 843, 844: 844, 845: 845, 846: 846, 847: 847, 848: 848, 849: 849, 850: 850, 851: 851, 852: 852, 853: 853, 854: 854, 855: 855, 856: 856, 857: 857, 858: 858, 859: 859, 860: 860, 861: 861, 862: 862, 863: 863, 864: 864, 865: 865, 866: 866, 867: 867, 868: 868, 869: 869, 870: 870, 871: 871, 872: 872, 873: 873, 874: 874, 875: 875, 876: 876, 877: 877, 878: 878, 879: 879, 880: 880, 881: 881, 882: 882, 883: 883, 884: 884, 885: 885, 886: 886, 887: 887, 888: 888, 889: 889, 890: 890, 891: 891, 892: 892, 893: 893, 894: 894, 895: 895, 896: 896, 897: 897, 898: 898, 899: 899, 900: 900, 901: 901, 902: 902, 903: 903, 904: 904, 905: 905, 906: 906, 907: 907, 908: 908, 909: 909, 910: 910, 911: 911, 912: 912, 913: 913, 914: 914, 915: 915, 916: 916, 917: 917, 918: 918, 919: 919, 920: 920, 921: 921, 922: 922, 923: 923, 924: 924, 925: 925, 926: 926, 927: 927, 928: 928, 929: 929, 930: 930, 931: 931, 932: 932, 933: 933, 934: 934, 935: 935, 936: 936, 937: 937, 938: 938, 939: 939, 940: 940, 941: 941, 942: 942, 943: 943, 944: 944, 945: 945, 946: 946, 947: 947, 948: 948, 949: 949, 950: 950, 951: 951, 952: 952, 953: 953, 954: 954, 955: 955, 956: 956, 957: 957, 958: 958, 959: 959, 960: 960, 961: 961, 962: 962, 963: 963, 964: 964, 965: 965, 966: 966, 967: 967, 968: 968, 969: 969, 970: 970, 971: 971, 972: 972, 973: 973, 974: 974, 975: 975, 976: 976, 977: 977, 978: 978, 979: 979, 980: 980, 981: 981, 982: 982, 983: 983, 984: 984, 985: 985, 986: 986, 987: 987, 988: 988, 989: 989, 990: 990, 991: 991, 992: 992, 993: 993, 994: 994, 995: 995, 996: 996, 997: 997, 998: 998, 999: 999, 1000: 1000}]

tests\conformance\runner.py:263: AssertionError
_________ TestConformance.test_yaml_case[limits::encode_binary_limit] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E797570>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='limits', description='Test various in-MOO limits on values (max_string_concat, max_list_value_byte...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = 3209, actual = 3207, test_name = 'encode_binary_limit'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'encode_binary_limit' expected value 3209, but got 3207

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[stress_objects::chparents_property_reset_multi] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E797D90>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected = [1, 1, 1], actual = [0, 0, 1]
test_name = 'chparents_property_reset_multi'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'chparents_property_reset_multi' expected value [1, 1, 1], but got [0, 0, 1]

tests\conformance\runner.py:285: AssertionError
_ TestConformance.test_yaml_case[stress_objects::object_bytes_permission_denied] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC4B0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_PERM'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'object_bytes_permission_denied'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_permission_denied' expected error E_PERM, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[stress_objects::object_bytes_wizard_allowed] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC500>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='object_bytes_wizard_allowed', description='', skip=False, skip_if=None, permission='wizard', setup=N...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_wizard_allowed' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
____ TestConformance.test_yaml_case[stress_objects::object_bytes_type_int] ____

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC550>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'object_bytes_type_int'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_type_int' expected error E_TYPE, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
___ TestConformance.test_yaml_case[stress_objects::object_bytes_type_float] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC5A0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'object_bytes_type_float'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_type_float' expected error E_TYPE, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
__ TestConformance.test_yaml_case[stress_objects::object_bytes_type_string] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC5F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_TYPE'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'object_bytes_type_string'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_type_string' expected error E_TYPE, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[stress_objects::object_bytes_recycled_object] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC640>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
expected_error = 'E_INVIND'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])
test_name = 'object_bytes_recycled_object'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_recycled_object' expected error E_INVIND, but got E_VERBNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[stress_objects::object_bytes_created_objects] _

self = <tests.conformance.test_conformance.TestConformance object at 0x000002410E7FC690>
runner = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
yaml_test_case = (MooTestSuite(name='stress::objects', description='Stress tests for object hierarchy operations - ancestor/descendant ...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x000002410E1C8440>
test = MooTestCase(name='object_bytes_created_objects', description='', skip=False, skip_if=None, permission='wizard', setup=...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'object_bytes_created_objects' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
=========================== short test summary info ===========================
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[caller_perms::caller_perms_top_level_eval]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::run_gc_requires_wizard_perms]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::run_gc_allows_wizard]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_requires_wizard_perms]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_allows_wizard]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_returns_map]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_has_purple_key]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_has_black_key]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_purple_is_int]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::gc_stats_black_is_int]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::nested_list_no_possible_root]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::nested_map_no_possible_root]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::run_gc_doesnt_crash_with_anonymous]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::single_cyclic_self_reference_basic]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::single_cyclic_self_reference_with_recycle]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::dual_cyclic_self_references]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::dual_cyclic_self_references_with_recycle]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::cyclic_references_through_list]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::cyclic_references_through_list_with_recycle]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::cyclic_references_through_map]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::cyclic_references_through_map_with_recycle]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::empty_list_is_green]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[gc::empty_map_is_green]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::non_wizard_cannot_call_no_arg_version]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::read_http_no_args_fails]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::read_http_invalid_type_foobar]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::read_http_invalid_type_empty_string]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::read_http_type_arg_not_string]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[http::read_http_connection_arg_not_obj]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[json::generate_json_escape_tab]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[json::generate_json_anon_obj]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[json::generate_json_anon_obj_common]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[json::generate_json_anon_obj_embedded]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[map::mapdelete_empty_list_key]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[map::mapdelete_list_values_key]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[map::ranged_set_invalid_range_2]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[map::ranged_set_invalid_range_3]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[map::inverted_ranged_set_in_loop]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_owner_invarg]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_owner_ambiguous]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_owner_failed_match]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_owner_invarg_as_programmer]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_parent_ambiguous]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_invalid_parent_failed_match]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_list_invalid_ambiguous]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::create_list_invalid_failed_match]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::renumber_basic]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[objects::chparent_property_conflict_variations]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[primitives::queued_tasks_includes_this_map]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[primitives::callers_includes_this_list]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[primitives::inheritance_with_prototypes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[primitives::pass_works_with_prototypes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_invalid_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_invalid_perms]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_builtin_name]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_defined_on_descendant]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::add_property_not_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::delete_property_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::is_clear_property_works]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::is_clear_property_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::is_clear_property_builtin]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::is_clear_property_with_read_permission]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::is_clear_property_wizard_bypasses_read]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::clear_property_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::clear_property_builtin]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::clear_property_on_definer]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::property_info_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::set_property_info_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[properties::properties_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[recycle::recycle_invalid_already_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[recycle::recycle_invalid_already_recycled_anonymous]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[switch_player::non_wizard_gets_E_PERM]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[switch_player::programmer_cannot_switch_player]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::fork_and_suspend_case]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::command_verb]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::server_verb]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::across_verb_calls]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::across_verb_calls_with_intermediate]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::suspend_between_verb_calls]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::read_between_verb_calls]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[task_local::nonfunctional_kill_task]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_basic]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_invalid_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_invalid_perms]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_invalid_args]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_with_write_permission]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_wizard_bypasses_write]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_not_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_is_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::add_verb_wizard_sets_owner]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::delete_verb_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::verb_info_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::verb_args_basic]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::verb_args_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::verb_code_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::set_verb_info_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::set_verb_args_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::set_verb_code_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verbs::verbs_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[anonymous::recycle_invalid_anonymous_no_crash]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[index_and_range::range_list_single]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[index_and_range::decompile_with_index_operators]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[waif::nested_waif_map_indexes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[waif::deeply_nested_waif_map_indexes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_with_sleep_works]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_rejects_invalid_binary_string]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_works_on_suspended_exec]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_fails_on_already_killed]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::resume_fails_on_suspended_exec]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::suspended_exec_task_stack_matches_suspended_task]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::setadd_checks_list_max_value_bytes_exceeds]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::listinsert_checks_list_max_value_bytes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::listappend_checks_list_max_value_bytes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::listset_fails_if_value_too_large]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::decode_binary_checks_list_max_value_bytes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::list_literal_checks_max_value_bytes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::map_literal_checks_max_value_bytes]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[limits::encode_binary_limit]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::chparents_property_reset_multi]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_permission_denied]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_wizard_allowed]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_type_int]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_type_float]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_type_string]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_recycled_object]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[stress_objects::object_bytes_created_objects]
128 failed, 1222 passed, 128 skipped in 12.43s
