============================= test session starts =============================
platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\Q\code\cow_py\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\Q\code\cow_py
configfile: pyproject.toml
collecting ... collected 1120 items / 1117 deselected / 3 selected

tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_prefix_matching] FAILED [ 33%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_full_name_matching] FAILED [ 66%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_partial_suffix_matching] FAILED [100%]

================================== FAILURES ===================================
_ TestConformance.test_yaml_case[verb_matching::wildcard_verb_prefix_matching] _

self = <tests.conformance.test_conformance.TestConformance object at 0x0000015B450A19F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
yaml_test_case = (MooTestSuite(name='verb_matching', description='Tests for verb name matching including wildcards', version='1.0', req...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:87: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
test = MooTestCase(name='wildcard_verb_prefix_matching', description='Verb with * wildcard matches abbreviated calls', skip=F...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=None, error_message='5', notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'wildcard_verb_prefix_matching' expected success but got error: 5

tests\conformance\runner.py:222: AssertionError
------------------------------ Captured log call ------------------------------
WARNING  cow_py.task_runner:task_runner.py:144 VM MOO error: code=5, msg=Verb not found
WARNING  cow_py.task_runner:task_runner.py:145   Frame: verb=, this=#0, ip=32
WARNING  cow_py.task_runner:task_runner.py:146   VM stack (top 10): [MOOList([ObjNum(8), 'test_func*tion', MOOList(['return 42;'])])]
WARNING  cow_py.task_runner:task_runner.py:150   Current instruction: Instruction(opcode=<Opcode.OP_BI_FUNC_CALL: 13>, operand=279, label=None, loop_var=None, loop_index=None, jump_target=None, handler_offset=None, error_codes=None, error_vars=None, scatter_pattern=None)
WARNING  cow_py.task_runner:task_runner.py:155   Variables: {'player': ObjNum(3), 'this': ObjNum(0), 'caller': ObjNum(0), 'verb': '', 'args': MOOList([])}
WARNING  cow_py.task_runner:task_runner.py:162   Call stack: ##0:
_ TestConformance.test_yaml_case[verb_matching::wildcard_verb_full_name_matching] _

self = <tests.conformance.test_conformance.TestConformance object at 0x0000015B450A1A40>
runner = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
yaml_test_case = (MooTestSuite(name='verb_matching', description='Tests for verb name matching including wildcards', version='1.0', req...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:87: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
test = MooTestCase(name='wildcard_verb_full_name_matching', description='Verb with * wildcard matches full name too', skip=Fa...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=None, error_message='5', notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'wildcard_verb_full_name_matching' expected success but got error: 5

tests\conformance\runner.py:222: AssertionError
------------------------------ Captured log call ------------------------------
WARNING  cow_py.task_runner:task_runner.py:144 VM MOO error: code=5, msg=Verb not found
WARNING  cow_py.task_runner:task_runner.py:145   Frame: verb=, this=#0, ip=32
WARNING  cow_py.task_runner:task_runner.py:146   VM stack (top 10): [MOOList([ObjNum(9), 'test_func*tion', MOOList(['return 99;'])])]
WARNING  cow_py.task_runner:task_runner.py:150   Current instruction: Instruction(opcode=<Opcode.OP_BI_FUNC_CALL: 13>, operand=279, label=None, loop_var=None, loop_index=None, jump_target=None, handler_offset=None, error_codes=None, error_vars=None, scatter_pattern=None)
WARNING  cow_py.task_runner:task_runner.py:155   Variables: {'player': ObjNum(3), 'this': ObjNum(0), 'caller': ObjNum(0), 'verb': '', 'args': MOOList([])}
WARNING  cow_py.task_runner:task_runner.py:162   Call stack: ##0:
_ TestConformance.test_yaml_case[verb_matching::wildcard_verb_partial_suffix_matching] _

self = <tests.conformance.test_conformance.TestConformance object at 0x0000015B450A1A90>
runner = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
yaml_test_case = (MooTestSuite(name='verb_matching', description='Tests for verb name matching including wildcards', version='1.0', req...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:87: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x0000015B44C302F0>
test = MooTestCase(name='wildcard_verb_partial_suffix_matching', description='Verb with * wildcard matches partial suffix', s...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=None, error_message='5', notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'wildcard_verb_partial_suffix_matching' expected success but got error: 5

tests\conformance\runner.py:222: AssertionError
------------------------------ Captured log call ------------------------------
WARNING  cow_py.task_runner:task_runner.py:144 VM MOO error: code=5, msg=Verb not found
WARNING  cow_py.task_runner:task_runner.py:145   Frame: verb=, this=#0, ip=32
WARNING  cow_py.task_runner:task_runner.py:146   VM stack (top 10): [MOOList([ObjNum(10), 'test_func*tion', MOOList(['return 77;'])])]
WARNING  cow_py.task_runner:task_runner.py:150   Current instruction: Instruction(opcode=<Opcode.OP_BI_FUNC_CALL: 13>, operand=279, label=None, loop_var=None, loop_index=None, jump_target=None, handler_offset=None, error_codes=None, error_vars=None, scatter_pattern=None)
WARNING  cow_py.task_runner:task_runner.py:155   Variables: {'player': ObjNum(3), 'this': ObjNum(0), 'caller': ObjNum(0), 'verb': '', 'args': MOOList([])}
WARNING  cow_py.task_runner:task_runner.py:162   Call stack: ##0:
============================== warnings summary ===============================
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_prefix_matching]
  C:\Users\Q\code\moo_interp\moo_interp\vm.py:146: UserWarning: Parameter(s) base, start, end, value of exec_rangeset are not annotated, will be considered as 'Any'
    warnings.warn(

tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_prefix_matching]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_full_name_matching]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_partial_suffix_matching]
  C:\Users\Q\code\moo_interp\moo_interp\vm.py:244: UserWarning: The following 1 opcodes are not implemented: ['OP_EXTENDED']
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ===========================
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_prefix_matching]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_full_name_matching]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[verb_matching::wildcard_verb_partial_suffix_matching]
=============== 3 failed, 1117 deselected, 4 warnings in 1.06s ================
