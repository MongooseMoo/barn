============================= test session starts =============================
platform win32 -- Python 3.13.5, pytest-9.0.2, pluggy-1.6.0 -- C:\Users\Q\code\cow_py\.venv\Scripts\python.exe
cachedir: .pytest_cache
rootdir: C:\Users\Q\code\cow_py
configfile: pyproject.toml
collecting ... collected 1479 items / 1456 deselected / 23 selected

tests/conformance/test_conformance.py::TestConformance::test_yaml_case[steps_basic::basic_step_execution] PASSED [  4%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_non_wizards] PASSED [  8%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_nonexistent_executables] PASSED [ 13%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_absolute_paths] PASSED [ 17%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_dot_slash_paths] PASSED [ 21%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_parent_dir_paths] PASSED [ 26%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_fails_for_path_traversal] PASSED [ 30%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_io_works] PASSED [ 34%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_args_works] PASSED [ 39%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_exit_status_works] PASSED [ 43%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_with_sleep_works] FAILED [ 47%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_rejects_invalid_binary_string] FAILED [ 52%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::queued_tasks_shows_suspended_exec] PASSED [ 56%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_works_on_suspended_exec] FAILED [ 60%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_fails_on_already_killed] FAILED [ 65%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::resume_fails_on_suspended_exec] FAILED [ 69%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::suspended_exec_task_stack_matches_suspended_task] FAILED [ 73%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::multiple_simultaneous_execs_work] SKIPPED [ 78%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::randomly_timed_execs_work] SKIPPED [ 82%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::multiple_fast_execs_work] SKIPPED [ 86%]
tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::fuzzy_input_does_not_break_exec] SKIPPED [ 91%]
tests/conformance/test_skip_evaluator.py::TestSkipConditionEvaluator::test_feature_exec_does_not_skip_when_we_lack_it PASSED [ 95%]
tests/conformance/test_skip_evaluator.py::TestSkipConditionEvaluator::test_not_feature_exec_skips_when_we_lack_it PASSED [100%]

================================== FAILURES ===================================
_________ TestConformance.test_yaml_case[exec::exec_with_sleep_works] _________

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D14A0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:238: in _verify_expectations
    self._verify_value(expect.value, result.value, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
expected = [0, '1~0A2~0A3~0A', ''], actual = [0, '1\r\n2\r\n3\r\n', '']
test_name = 'exec_with_sleep_works'

    def _verify_value(self, expected: Any, actual: Any, test_name: str) -> None:
        """Verify exact value match."""
        if not self._values_equal(expected, actual):
>           raise AssertionError(
                f"Test '{test_name}' expected value {expected!r}, "
                f"but got {actual!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'exec_with_sleep_works' expected value [0, '1~0A2~0A3~0A', ''], but got [0, '1\r\n2\r\n3\r\n', '']

tests\conformance\runner.py:285: AssertionError
__ TestConformance.test_yaml_case[exec::exec_rejects_invalid_binary_string] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D14F0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=True, value=[0, '1~ZZ23~0A', '1~ZZ23~0A'], error=None, error_message=None, notifications=[], logs=[])
test_name = 'exec_rejects_invalid_binary_string'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
E           tests.conformance.runner.AssertionError: Test 'exec_rejects_invalid_binary_string' expected error E_INVARG, but got success with value: [0, '1~ZZ23~0A', '1~ZZ23~0A']

tests\conformance\runner.py:263: AssertionError
___ TestConformance.test_yaml_case[exec::kill_task_works_on_suspended_exec] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D1590>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
test = MooTestCase(name='kill_task_works_on_suspended_exec', description='', skip=False, skip_if=None, permission='wizard', s...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'kill_task_works_on_suspended_exec' expected success but got error: MooError.E_VARNF

tests\conformance\runner.py:231: AssertionError
___ TestConformance.test_yaml_case[exec::kill_task_fails_on_already_killed] ___

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D15E0>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])
test_name = 'kill_task_fails_on_already_killed'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'kill_task_fails_on_already_killed' expected error E_INVARG, but got E_VARNF

tests\conformance\runner.py:277: AssertionError
____ TestConformance.test_yaml_case[exec::resume_fails_on_suspended_exec] _____

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D1630>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...', type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
tests\conformance\runner.py:226: in _verify_expectations
    self._verify_error(expect.error, result, test.name)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
expected_error = 'E_INVARG'
result = ExecutionResult(success=False, value=None, error=<MooError.E_VARNF: 'E_VARNF'>, error_message=None, notifications=[], logs=[])
test_name = 'resume_fails_on_suspended_exec'

    def _verify_error(self, expected_error: str, result: ExecutionResult, test_name: str) -> None:
        """Verify that an error was returned."""
        if result.success:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got success with value: {result.value!r}"
            )
    
        if result.error is None:
            raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got non-MOO error: {result.error_message}"
            )
    
        # Compare error codes
        actual_error = result.error.value if isinstance(result.error, MooError) else str(result.error)
        if actual_error != expected_error:
>           raise AssertionError(
                f"Test '{test_name}' expected error {expected_error}, "
                f"but got {actual_error}"
            )
E           tests.conformance.runner.AssertionError: Test 'resume_fails_on_suspended_exec' expected error E_INVARG, but got E_VARNF

tests\conformance\runner.py:277: AssertionError
_ TestConformance.test_yaml_case[exec::suspended_exec_task_stack_matches_suspended_task] _

self = <tests.conformance.test_conformance.TestConformance object at 0x00000163E12D1680>
runner = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
yaml_test_case = (MooTestSuite(name='exec', description='Tests for exec() builtin - external process execution', version='1.0', skip=Fa...e, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000))

    def test_yaml_case(self, runner: YamlTestRunner, yaml_test_case: tuple[MooTestSuite, MooTestCase]):
        """Run a single YAML test case.
    
        This method is called once for each test case defined in YAML files.
        The yaml_test_case fixture is parametrized by conftest.pytest_generate_tests.
        """
        suite, test = yaml_test_case
    
        # Handle skip
        if test.skip:
            reason = test.skip if isinstance(test.skip, str) else "Skipped"
            pytest.skip(reason)
    
        # Handle skip_if condition
        if test.skip_if:
            if self._evaluate_skip_condition(test.skip_if):
                pytest.skip(f"Skipped due to: {test.skip_if}")
    
        # Run suite setup if defined (only runs once per suite)
        runner.run_suite_setup(suite)
    
        # Run the test
>       runner.run_test(test)

tests\conformance\test_conformance.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests\conformance\runner.py:96: in run_test
    self._verify_expectations(test, result)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <tests.conformance.runner.YamlTestRunner object at 0x00000163E0D0C440>
test = MooTestCase(name='suspended_exec_task_stack_matches_suspended_task', description='', skip=False, skip_if=None, permiss...ne, type=None, match=None, contains=None, range=None, satisfies=None, notifications=None), cleanup=[], timeout_ms=5000)
result = ExecutionResult(success=False, value=None, error=<MooError.E_VERBNF: 'E_VERBNF'>, error_message=None, notifications=[], logs=[])

    def _verify_expectations(self, test: MooTestCase, result: ExecutionResult) -> None:
        """Verify test result against expectations.
    
        Args:
            test: The test case with expectations
            result: The execution result to verify
    
        Raises:
            AssertionError: If any expectation is not met
        """
        expect = test.expect
    
        # Check for expected error
        if expect.error:
            self._verify_error(expect.error, result, test.name)
            return
    
        # If we got here, we expect success
        if not result.success:
>           raise AssertionError(
                f"Test '{test.name}' expected success but got error: "
                f"{result.error or result.error_message}"
            )
E           tests.conformance.runner.AssertionError: Test 'suspended_exec_task_stack_matches_suspended_task' expected success but got error: MooError.E_VERBNF

tests\conformance\runner.py:231: AssertionError
=========================== short test summary info ===========================
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_with_sleep_works]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::exec_rejects_invalid_binary_string]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_works_on_suspended_exec]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::kill_task_fails_on_already_killed]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::resume_fails_on_suspended_exec]
FAILED tests/conformance/test_conformance.py::TestConformance::test_yaml_case[exec::suspended_exec_task_stack_matches_suspended_task]
========== 6 failed, 13 passed, 4 skipped, 1456 deselected in 6.90s ===========
